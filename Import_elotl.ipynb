{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umoqnier/Esquite/blob/notebook-test/Import_elotl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjmENg7p46R6"
      },
      "source": [
        "# **Escuela de Verano PLN 2020**. Sesión práctica, parte 1\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQoRrzkoWKfD"
      },
      "source": [
        "# Cargamos corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a65pnCPxcl4a"
      },
      "source": [
        "**Vamos a utilizar esta biblioteca (Elotl) que permite trabajar con diversos corpus paralelos para lenguas habladas en México:**\n",
        "\n",
        "[Consultar Documentación](https://pypi.org/project/elotl/)\n",
        "\n",
        "[Comunidad Elotl](https://elotl.mx/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZOBPJ8_ZFG2"
      },
      "source": [
        "!pip install elotl  #Tenemos que instalar la paquetería elotl desde pip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjAMErJWlTHm"
      },
      "source": [
        "#Paqueterías (ya instaladas) de Python que utilizaremos a lo largo de los ejercicios\n",
        "from collections import defaultdict, Counter\n",
        "from re import sub\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Pd7iDMV17dH"
      },
      "source": [
        "Importamos un corpus paralelo **español-náhuatl** ([Axolotl](https://axolotl-corpus.mx/)) y uno **español-otomí** ([Tsunkua](https://tsunkua.elotl.mx/))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voAOZCq8az0B"
      },
      "source": [
        "import elotl.corpus\n",
        "#1. El método corpus.load() nos regresa una lista multidimensional. Cada elemento de esta lista corresponde a una oración paralela del corpus más información extra del par de oraciones\n",
        "corpus1=elotl.corpus.load('axolotl')  #español-náhuatl\n",
        "corpus2=elotl.corpus.load('tsunkua')  #español-otomi\n",
        "#Contenido de cada elemento de la lista:\n",
        "  #Oración en Lengua 1 (español). Ejemplo: corpus1[0][0]\n",
        "  #Oración en Lengua 2. Ejemplo: corpus1[0][1]\n",
        "  #Variante de la lengua 2. Ejemplo: corpus1[0][2]\n",
        "  #Nombre del documento de dóónde fue extraido este fragmento paralelo. Ejemplo: corpus1[0][3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3YgbexHvpTd"
      },
      "source": [
        "# Estadísticas generales\n",
        "\n",
        "**- Número de oraciones paralelas**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ElCRLT1R7Fu"
      },
      "source": [
        "#1. El método len() de python regresa el número de elementos que tiene un objeto, en nuestro caso queremos saber cuántos elementos tienen las listas que contienen al corpus1 y al corpus2, respectivamente:\n",
        "size_corpus1=len(corpus1)\n",
        "size_corpus2=len(corpus2)\n",
        "print(\"Oraciones paralelas en corpus español-náhuatl:\", size_corpus1)\n",
        "print(\"Oraciones paralelas en corpus español-otomí:\", size_corpus2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGasRqAZXBPJ"
      },
      "source": [
        "**- Número total de palabras por lengua (tokens)**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeMbAl4BwHep"
      },
      "source": [
        "#1. Vamos a extraer listas de palabras para cada lengua (a partir de los corpus paralelos):\n",
        "corpus1_es=[] #Lista de palabras en español del corpus 1 (español-náhuatl)\n",
        "corpus1_na=[] #Lista de palabras en náhuatl del corpus 1 (español-náhuatl)\n",
        "corpus2_es=[] #Lista de palabras en español del corpus 2 (español-otomí)\n",
        "corpus2_ot=[] #Lista de palabras en otomí del corpus 2 (español-otomí)\n",
        "words=[] #Lista temporal\n",
        "\n",
        "#2. Vamos a iterar sobre cada línea de los corpus paralelos e ir guardando las palabras que corresponden a cada lengua:\n",
        "#2.1 Por cada línea del Corpus español-náhuatl\n",
        "for row in corpus1:\n",
        "  #2.1.1 Vamos con la oración en español (row[0]):\n",
        "  row[0]=sub(r'[^\\w\\s]',' ',row[0])     #Sustituimos signos de puntuación por un espacio\n",
        "  words=row[0].lower().split()          #Dividimos a la oración en palabras (el delimitador es un espacio), guardamos cada una de estas palabras como elementos de la lista words\n",
        "  for w in words:                       #Iteramos sobre cada palabra de la oración (lista words)\n",
        "       corpus1_es.append(w)             #vamos agregando cada palabra a una lista (corpus1_es), que al final contendrá todas las palabras en español del corpus1\n",
        "  #2.1.2 Vamos con la oración en nááhuatl (row[1]):\n",
        "  row[1]=sub(r'[^\\w\\s\\']',' ',row[1])  #Sustituir signos de puntuación. Para el caso del náhuatl y el otomí se podríían estar filtrando caracteres que no son signos de puntuación sino marcas de tono o consonantes. Por ejemplo \"'\"\n",
        "  words=row[1].lower().split()         #Dividimos a la oración en palabras (el delimitador es un espacio), guardamos cada una de estas palabras como elementos de la lista words\n",
        "  for w in words:                      #Iteramos sobre cada palabra de la oración (lista words)\n",
        "       corpus1_na.append(w)             #vamos agregando cada palabra a una lista (corpus1_na), que al final contendrá todas las palabras en náhuatl del corpus1\n",
        "\n",
        "#2.2 Por cada línea del Corpus español-otomí  (mismo procedimiento)\n",
        "for row in corpus2:\n",
        "  row[0]=sub(r'[^\\w\\s]',' ',row[0])\n",
        "  words=row[0].lower().split()\n",
        "  for w in words:\n",
        "       corpus2_es.append(w)\n",
        "\n",
        "  row[1]=sub(r'[^\\w\\s\\']',' ',row[1])\n",
        "  words=row[1].lower().split()\n",
        "  for w in words:\n",
        "       corpus2_ot.append(w)\n",
        "\n",
        "#3. Calculamos el núúmero total de palabras utilizando la función len() sobre la listas de palabras que creamos:\n",
        "\n",
        "print (\"Número total de palabras en náhuatl (corpus1):\", len(corpus1_na))\n",
        "print (\"Número total de palabras en español (corpus1):\", len(corpus1_es))\n",
        "print (\"Número total de palabras en otomí (corpus2):\", len(corpus2_ot))\n",
        "print (\"Número total de palabras en español (corpus2):\", len(corpus2_es))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3dgw3gG_U5B"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "*   **Discusión:** ¿Por qué si los textos son paralelos (traducciones) el número de palabras cambia tanto entre lenguas? e.g., el español tiene ~100,000 palabras más que el náhuatl (corpus 1)\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0tWuDX0hZY-"
      },
      "source": [
        "En NLP, al número total de palabras dentro de un texto (incluidas repeticiones) también se les conoce como número de **TOKENS**. Mientras que al número de palabras distintas se le conoce como número de **TIPOS** o  tamaño del vocabulario.\n",
        "\n",
        "**- Frecuencia de las palabras y número de tipos por lengua**\n",
        "\n",
        "Vamos a calcular las frecuencias de las palabras en cada lengua de una manera muy práctica. Utilizaremos una función de Python Counter() que regresa automáticamente las frecuencias de cada elemento de una lista. [Documentación](https://docs.python.org/3/library/collections.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suBBj3KY7szg"
      },
      "source": [
        "#1. Le pasamos a la función Counter(), la lista de palabras que obtuvimos para cada lengua/corpus. La función nos regresará un tipo de diccionario (Python) donde cada elemento es un par: (palabra, frecuencia)\n",
        "corpus1_na_voc=Counter(corpus1_na)\n",
        "corpus1_es_voc=Counter(corpus1_es)\n",
        "corpus2_ot_voc=Counter(corpus2_ot)\n",
        "corpus2_es_voc=Counter(corpus2_es)\n",
        "\n",
        "#2. Una vez obtenidos los diccionarios de frecuencias para cada lenguas, podemos utilizar varias de las funciones que permite Counter(). Por ejemplo, obtener los elementos más frecuentes most_common()\n",
        "corpus1_es_voc.most_common()[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2N1HGayNOI6L"
      },
      "source": [
        "#3. Si contamos el número de elemento en el diccionario de frecuencias obtendremos el tamaño del vocabulario o número de tipos\n",
        "print (\"Tamaño del vocabulario de náhuatl (corpus1):\", len(corpus1_na_voc))\n",
        "print (\"Tamaño del vocabulario de español (corpus1):\", len(corpus1_es_voc))\n",
        "print (\"Tamaño del vocabulario de otomí (corpus2):\", len(corpus2_ot_voc))\n",
        "print (\"Tamaño del vocabulario de español (corpus2):\", len(corpus2_es_voc))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJFSBRYW6wOC"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "*   **Discusión:** ¿Cómo cambiarían estas estadísticas si no filtramos los signos de puntuación?\n",
        "\n",
        "¿Cómo afecta la falta de normalización ortográfica en lenguas como el náhuatl?\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9PzAC9dXV_-"
      },
      "source": [
        "**- Longitud promedio de palabra para el español, otomí, náhuatl**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzFpGj_s9_MA"
      },
      "source": [
        " #1. Calculamos la longitud de cada token para cada lengua y sacamos un promedio\n",
        "  #Para evitar hacer código repetitivo, creamos una sola función que toma como entrada una lista de tokens y devuelve la longitud promedio:\n",
        " def avg_length(tokens): #lista de tokens\n",
        "   total=0 #variable auxiliar\n",
        "   for w in tokens:       #Por cada token de la lengua\n",
        "    size=len(w)           #Devuelve el número de caracteres que posee el token\n",
        "    total=total+size      #Por cada nuevo token vamos sumando su longitud\n",
        "   avg=total/len(tokens)  #Calcular Promedio\n",
        "   return avg\n",
        "\n",
        "print (\"Longitud promedio de palabra en náhuatl (corpus1):\", avg_length(corpus1_na))\n",
        "print (\"Longitud promedio de palabra en español (corpus1):\", avg_length(corpus1_es))\n",
        "print (\"Longitud promedio de palabra en otomí (corpus2):\", avg_length(corpus2_ot))\n",
        "print (\"Longitud promedio de palabra en español (corpus2):\", avg_length(corpus2_es))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQuMt-FZwHx4"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*Ejercicio*:\n",
        "¿Cuál es la palabra más larga del náhuatl en el corpus?\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFd1r4c-JYjC"
      },
      "source": [
        "# Visualización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgutKfl1Du6K"
      },
      "source": [
        "- **Frecuencias**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C39DwU-WFeCP"
      },
      "source": [
        "plt.rcParams['figure.figsize'] = [15, 6]\n",
        "\n",
        "#1. Diseñamos una función que recibe un diccionario de palabra,frecuencia (c) y un umbral (n). Nos regresa una lista con las (n) frecuencias más altas del diccionario\n",
        "def get_freqs(c, n): #counter and number of terms\n",
        "  freqs=[]\n",
        "  pairs=c.most_common(n)\n",
        "  for p in pairs:\n",
        "    freqs.append(p[1])\n",
        "  return freqs\n",
        "\n",
        "#2. Aplicamos la función\n",
        "frequency=get_freqs(corpus1_na_voc, 300)    #Obtenemos las n frecuencias más altas (ordenadas) y las guardamos en una lista\n",
        "x = list(range(1, 301))                     #Generamos una lista con números del 1 al n. Esto será el eje de las X en la gráfica y servirá para indicar el ranking de cada frecuencia (la frecuencia más grande ocupa el ranking 1 y así sucesivamente)\n",
        "#p=np.array(frequency)\n",
        "#prob=p/p.sum()\n",
        "\n",
        "#3. Visualizamos con ayuda de matplotlib, y las dos listas que generamos: frequency, x\n",
        "plt.plot(x, frequency,'-v')\n",
        "plt.xlabel('Frequency rank (r)')\n",
        "plt.ylabel('Frequency (f)')\n",
        "plt.title('Frequency of words')\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqxbm1dRKGUZ"
      },
      "source": [
        "**- Ley de Zipf**\n",
        "\n",
        "Podemos ver que las frecuencias en las diferentes lenguas siguen un patrón parecido. Pocos palabras (tipos) son muy frecuentes, mientras que la mayoría de palabras ocurren con frecuencia baja.\n",
        "\n",
        "De hecho, la frecuencia de la palabra que ocupa la posición r en el rank, es proporcional a 1/r (La palabra más frecuente ocurrirá aproximadamente el doble de veces que la segunda palabra más frecuente en el corpus y tres veces más que la tercer palabra más frecuente del corpus,...)\n",
        "\n",
        "![Zipf.png](data:image/png;base64,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)\n",
        "\n",
        "*r* es el rank de frecuencia que ocupa una palabra en el corpus\n",
        "\n",
        "*f(r)* es la frecuencia de esa palabra\n",
        "\n",
        "*alpha* es un parámetro, su valor puede cambiar dependiendo del corpus o fenómeno que estemos observando"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwdL4KcUpti6"
      },
      "source": [
        "#1. Vamos a generar una lista de números aleatorios que sigan una distribución de Zipf\n",
        "a = 2                           # parametro alpha*\n",
        "s = np.random.zipf(a, 286805)   #Número de elementos que queremos generar\n",
        "s2=Counter(s)                   #Usando Counter obtenemos un diccionario de las frecuencias de cada número generado\n",
        "\n",
        "#2. Extraemos las frecuencias de los primeros n elementos\n",
        "frequency2=get_freqs(s2, 300)\n",
        "\n",
        "#3. Visualizamos las frecuencias y su rank usando matplotlib:\n",
        "plt.plot(x, frequency2,'-v')\n",
        "plt.xlabel('Frequency rank')\n",
        "plt.ylabel('frequency')\n",
        "plt.title('Frequency of words')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1V3j4jZxpVt"
      },
      "source": [
        "#4. Juntamos las dos curvas (corpus real y números aleatorios generados con Zipf):\n",
        "plt.figure()\n",
        "plot1=plt.plot(x, frequency,'-v')\n",
        "plot2=plt.plot(x, frequency2,'-v')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCK-9hN8bp95"
      },
      "source": [
        "**- Calcular la longitud promedio de las palabras en la \"cabeza\" y en la \"cola\" de la distribución**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdcqtY4jgDfU"
      },
      "source": [
        " # 1. Creamos dos funciones para extraer n palabras de la cabeza (más frecuentes) y n palabras de la cola (menos frecuentes)\n",
        "\n",
        " #1.1 Función para extraer las n palabras menos frecuentes (cola)\n",
        " def get_words_tail(c, n):                   #Recibe un diccionario (c) y el número de elementos que queremos extraer (n)\n",
        "  words=[]\n",
        "  pairs=c.most_common()[:-n-1:-1]\n",
        "  for p in pairs:\n",
        "    words.append(p[0])\n",
        "  return words                                 #Regresa una lista de palabras\n",
        "\n",
        " #1.2 Función para extraer las n palabras más frecuentes (cabeza)\n",
        " def get_words_head(c, n):                      #Recibe un diccionario (c) y el número de elementos que queremos extraer (n)\n",
        "  words=[]\n",
        "  pairs=c.most_common(n)\n",
        "  for p in pairs:\n",
        "    words.append(p[0])\n",
        "  return words                                  #Regresa una lista de palabras\n",
        "\n",
        "\n",
        "words_head=get_words_head(corpus2_ot_voc, 19)   #Obtenemos la lista de n palabras más frecuentes (cabeza de la distribución)\n",
        "words_tail=get_words_tail(corpus2_ot_voc, 19)   #Obtenemos la lista de n palabras menos frecuentes (cola de la distribución)\n",
        "\n",
        "#2. Utilizamos la función avg_length(), que realizamos anteriormente en otra celda, para calcular la longitud promedio de las palabras en una lista:\n",
        "print (\"Longitud promedio de las palabras más frecuentes:\", avg_length(words_head))\n",
        "print (\"Longitud promedio de las palabras menos frecuentes:\", avg_length(words_tail))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_sPCZsFkf78"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "*   **Discusión:** ¿Por qué las palabras más frecuentes son más cortas?\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CYxCiqHk7gD"
      },
      "source": [
        "**- Nubes de palabras**\n",
        "\n",
        "Ocuparemos la biblioteca WordCloud, que permite diversas opciones para personalizar  nubes de palabras basadas en frecuencias. [Ver documentación](https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj52IhdiG6_k"
      },
      "source": [
        "from wordcloud import WordCloud  #Biblioteca\n",
        "#1. Inicializamos un objeto WordCloud, se pueden establecer diversos parámetros para personalizar la nube. Nosotros solo establecemos una instancia con los paráámetros default ()\n",
        "mycloud = WordCloud()\n",
        "\n",
        "#2. Le pasamos al objeto los datos para alimentar a la nube. En nuestro caso, un diccionario de palabras con su frecuencia. (también se le puede pasar directamente un texto y la biblioteca calculará las frecuencias)\n",
        "mycloud.generate_from_frequencies(corpus2_ot_voc)\n",
        "\n",
        "#3. Visualizamos nuestro objeto con ayuda de matplotlib\n",
        "plt.figure(figsize=(8,6), dpi=120)\n",
        "plt.imshow(mycloud)\n",
        "plt.axis(\"off\")\n",
        "plt.savefig('/content/nube_otomi.png') #Si queremos guardar la imagen en el drive, aquí específicamos la ruta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyInoppd4zJG"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "*Ejercicio*:\n",
        "Generar y guardar las nubes de palabras para todos los corpus, con diferentes parámetros\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxTQMB4E46yN"
      },
      "source": [
        "Las nubes están llenas de **palabras funcionales** o **\"stop words\"**, es decir, palabras que no tienen tanto contenido semáántico y que más bien tienen una función gramatical.\n",
        "\n",
        "En NLP suelen eliminarse, esto constituye un paso de **pre-procesamiento** típico. Es por esto que existen diversas herramientas que facilitan este tipo de tareas. Veremos el caso de la **biblioteca NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbCKFyYUAywM"
      },
      "source": [
        "import nltk                      #Biblioteca para tareas de PLN\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')       #Descargar lista de stopwords o palabras funcionales para muchas lenguas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck40ZbXCBgXm"
      },
      "source": [
        "#1. Utilizando NLTK obtendremos una lista pre-compilada de stop-words para el español.\n",
        "language = \"spanish\"     #Definimos una lengua para la cual queremos filtrar las palabras funcionales\n",
        "palabrasfuncionales = stopwords.words(language) #Usando la función stopwords de nltk generamos una lista de palabras funcionales comunes del español\n",
        "\n",
        "#2. Usando nuestro diccionario (donde guardamos las palabras y frecuencias del español), nos quedaremos solo con aquellas palabras que NO aparezcan en la lista palabrasfuncionales:\n",
        "corpus1_es_voc_filtered={a: b for a, b in corpus1_es_voc.items() if a not in palabrasfuncionales}\n",
        "\n",
        "#3. Generamos un nuevo objeto WordCloud con el nuevo diccionario sin palabras funcionales/stopwords:\n",
        "cloud_filtered = WordCloud(width=500, height=200)\n",
        "cloud_filtered.generate_from_frequencies(corpus1_es_voc_filtered)\n",
        "\n",
        "#4. Visualizamos nuestro objeto con ayuda de matplotlib\n",
        "plt.figure(figsize=(8,6), dpi=120)\n",
        "plt.imshow(cloud_filtered, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.savefig('/content/nube_esp_filtered.png') #Si queremos guardar la imagen en el drive, aquí específicamos la ruta\n",
        "plt.show() #Esta función muestra la imagen generada"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1lQ3NdWU5nH"
      },
      "source": [
        "texto en cursiva---\n",
        "\n",
        "\n",
        "*   **Discusión:** ¿Qué podemos hacer para el caso del náhuatl y el otomi si no tienen paquetería especializada para realizar este filtrado de stop words?\n",
        "\n",
        "\n",
        "---"
      ]
    }
  ]
}